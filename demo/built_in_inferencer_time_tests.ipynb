{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Time Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"*/usr/local/lib/python3.10/dist-packages/mmengine/model/base_model/base_model.py*\" file (mmengine) with class BaseModel add **imports** \n",
    "\n",
    "```python\n",
    "import time\n",
    "from demo.json_handler import JSONHandler\n",
    "import torch\n",
    "```\n",
    "\n",
    "and **the following** (in test_step):\n",
    "\n",
    "```python\n",
    "        torch.cuda.synchronize()\n",
    "        begin = time.time()\n",
    "        data = self.data_preprocessor(data, False)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "        out_file = '/home/michele/code/michele_mmdet3d/data/minerva_polimove/inference_times.json'\n",
    "        handler = JSONHandler(out_file)\n",
    "        handler.update_dictionary({'Pre-processing delta_t': (end-begin)})\n",
    "\n",
    "        return self._run_forward(data, mode='predict')  # type: ignore\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                      INFERENCE AND SAVE THE DATA                                              #\n",
    "#################################################################################################################\n",
    "\n",
    "# Needed imports\n",
    "import sys\n",
    "from json_handler import JSONHandler\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "# Add the new paths separately\n",
    "new_path2 = \"/home/michele/code/michele_mmdet3d/\"\n",
    "if not new_path2 in sys.path:\n",
    "    sys.path.insert(1, new_path2)\n",
    "# Import the fucking wicked class\n",
    "from mmdet3d.apis import LidarDet3DInferencer\n",
    "\n",
    "\n",
    "# Initialize the actual inferencer class\n",
    "inferencer = LidarDet3DInferencer(model='/home/michele/code/michele_mmdet3d/configs/minerva/CONDENSED_pointpillars_minerva.py', \n",
    "                                  weights='/home/michele/code/michele_mmdet3d/work_dirs/pointpillars_minerva/epoch_120.pth',\n",
    "                                  show_progress=False)\n",
    "\n",
    "\n",
    "# Read the files in validation list\n",
    "val_list_txt_file = \"/home/michele/code/michele_mmdet3d/data/minerva_polimove/ImageSets/val.txt\"\n",
    "with open(val_list_txt_file, 'r') as file:\n",
    "    val_file_names = [line.strip() for line in file]\n",
    "print(f\"Total validation point_clouds: {len(val_file_names)}\")\n",
    "\n",
    "\n",
    "# Create the list of inputs, suitable for the inferencer\n",
    "inputs = []\n",
    "for i, name in enumerate(val_file_names):\n",
    "    inputs.append(dict(points=(\"/home/michele/code/michele_mmdet3d/data/minerva_polimove/training/velodyne/\"+name+\".bin\")))\n",
    "\n",
    "\n",
    "# Create an instance JSONHandler\n",
    "out_file = '/home/michele/code/michele_mmdet3d/data/minerva_polimove/inference_times.json'\n",
    "handler = JSONHandler(out_file)\n",
    "# Remove the file if already existing\n",
    "handler.reset()\n",
    "# Do the actual inference\n",
    "results = []\n",
    "for i, input in enumerate(inputs):\n",
    "    begin = time.time()\n",
    "    handler.add_dictionary({'Everything start': begin})\n",
    "    results.append(inferencer(input))\n",
    "    end = time.time()\n",
    "    handler.update_dictionary({'Everything end': end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in handler.read_json_file():\n",
    "    print(element)\n",
    "\n",
    "dictionary_list = handler.read_json_file()\n",
    "print(f\"\\nThere are {len(dictionary_list)} elements in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                  STUDY OF BASE_MODEL (NOT ACCESSIBLE)                                         #\n",
    "#################################################################################################################\n",
    "\n",
    "start = handler.get_parameter('Everything start')\n",
    "end = handler.get_parameter('Voxel encoder start')\n",
    "estimated_delta = []\n",
    "for i in range(len(start)):\n",
    "    estimated_delta.append( end[i]-start[i] )\n",
    "\n",
    "compute_error = False\n",
    "\n",
    "if compute_error:\n",
    "    actual_delta = handler.get_parameter('Pre-processing delta_t')\n",
    "    error = []\n",
    "    error_abs = []\n",
    "    for i in range(len(actual_delta)):\n",
    "        error.append( actual_delta[i]-estimated_delta[i] )\n",
    "        error_abs.append( abs( actual_delta[i]-estimated_delta[i] ) )\n",
    "\n",
    "av_est_deltaT = sum(estimated_delta)/len(estimated_delta)\n",
    "print(f\"Average value of the estimated delta_t: {av_est_deltaT}\")\n",
    "if compute_error:\n",
    "    print(f\"Average value of the error: {sum(error)/len(error)}\")\n",
    "    av_abs_error = sum(error_abs)/len(error_abs)\n",
    "    print(f\"Average value for the absolute error: {av_abs_error}\")\n",
    "    av_act_deltat = sum(actual_delta)/len(actual_delta)\n",
    "    print(f\"The percentage of error is {av_abs_error/av_est_deltaT*100}. The average actual delta_t is {av_act_deltat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                            GENERAL STUDY OF TIME                                              #\n",
    "#################################################################################################################\n",
    "\n",
    "start_everything = handler.get_parameter('Everything start')\n",
    "start_voxel = handler.get_parameter('Voxel encoder start')\n",
    "delta_voxel = handler.get_parameter('Voxel encoder delta_t')\n",
    "delta_middle = handler.get_parameter('Middle encoder delta_t')\n",
    "delta_backbone = handler.get_parameter('Backbone delta_t')\n",
    "delta_neck = handler.get_parameter('Neck delta_t')\n",
    "delta_post = handler.get_parameter('Post-processing delta_t')\n",
    "end_everything = handler.get_parameter('Everything end')\n",
    "\n",
    "# Total time\n",
    "delta_total = []\n",
    "for i in range(len(start_everything)):\n",
    "    delta_total.append(end_everything[i]-start_everything[i])\n",
    "estimated_total = sum(delta_total)/len(delta_total)\n",
    "print(f\"Average total time: {estimated_total*1e3:.2f}ms which would be {1/estimated_total:.2f}Hz!\")\n",
    "# Preprocessing\n",
    "delta_pre = []\n",
    "for i in range(len(start_everything)):\n",
    "    delta_pre.append(start_voxel[i]-start_everything[i])\n",
    "estimated_pre = sum(delta_pre)/len(delta_pre)\n",
    "print(f\"\\tAverage pre-processing time:\\t{estimated_pre*1e3:.2f}ms which means\\t{100*estimated_pre/estimated_total:.2f}% over the total. \\\n",
    "      \\n\\t\\tUsually 10% higher than actual value, which would be {estimated_pre*0.90*1e3:.2f}ms\")\n",
    "# Voxel encoder\n",
    "estimated_voxel = sum(delta_voxel)/len(delta_voxel)\n",
    "print(f\"\\tAverage voxel encoder time:\\t{estimated_voxel*1e3:.2f}ms which means\\t{100*estimated_voxel/estimated_total:.2f}% over the total.\")\n",
    "# Middle encoder\n",
    "estimated_middle = sum(delta_middle)/len(delta_middle)\n",
    "print(f\"\\tAverage middle encoder time:\\t{estimated_middle*1e3:.2f}ms which means\\t{100*estimated_middle/estimated_total:.2f}% over the total.\")\n",
    "# Backbone\n",
    "estimated_backbone = sum(delta_backbone)/len(delta_backbone)\n",
    "print(f\"\\tAverage backbone time:\\t\\t{estimated_backbone*1e3:.2f}ms which means\\t{100*estimated_backbone/estimated_total:.2f}% over the total.\")\n",
    "# Neck\n",
    "estimated_neck = sum(delta_neck)/len(delta_neck)\n",
    "print(f\"\\tAverage neck time:\\t\\t{estimated_neck*1e3:.2f}ms which means\\t{100*estimated_neck/estimated_total:.2f}% over the total.\")\n",
    "# Post-processing\n",
    "estimated_post = sum(delta_post)/len(delta_post)\n",
    "print(f\"\\tAverage post-processing time:\\t{estimated_post*1e3:.2f}ms which means\\t{100*estimated_post/estimated_total:.2f}% over the total.\")\n",
    "# Final comment\n",
    "sum_all_0 = estimated_pre + estimated_voxel + estimated_middle + estimated_backbone + estimated_neck + estimated_post\n",
    "sum_all_1 = estimated_pre*0.90 + estimated_voxel + estimated_middle + estimated_backbone + estimated_neck + estimated_post\n",
    "sum_inference = estimated_voxel + estimated_middle + estimated_backbone + estimated_neck\n",
    "print(f\"\\nBy summing all the \\\"sub-components\\\" we get:\\\n",
    "      \\n\\tA total time of {sum_all_0*1e3:.2f}ms ({1/sum_all_0:.2f}Hz) which means\\t{100*sum_all_0/estimated_total:.2f}% over the total of\\t{estimated_total*1e3:.2f}ms\\\n",
    "      \\nand by considering the \\\"real\\\" value for the preprocessing (9.85% less), we get:\\\n",
    "      \\n\\tA total time of {sum_all_1*1e3:.2f}ms ({1/sum_all_1:.2f}Hz) which means\\t{100*sum_all_1/estimated_total:.2f}% over the total of\\t{estimated_total*1e3:.2f}ms\\\n",
    "      \\nwhile the inference (from voxel encoder to neck) accounts for:\\\n",
    "      \\n\\tA total time of {sum_inference*1e3:.2f}ms which means\\t\\t{100*sum_inference/estimated_total:.2f}% over the total of\\t{estimated_total*1e3:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                            PLOTTING OF FREQUENCY                                              #\n",
    "#################################################################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_frequency_plot(values, title = \"Frequency Plot\"):\n",
    "    # Calculating the average value of the given list\n",
    "    average_value = sum(values) / len(values)\n",
    "\n",
    "    # Plotting the frequency plot with a vertical red line for the average value\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(values, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.axvline(average_value, color='red', linestyle='dashed', linewidth=2, label=f'Average: {average_value:.4f}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# delta_inference = []\n",
    "# for i in range(len(delta_voxel)):\n",
    "#     delta_inference.append(delta_voxel[i] + delta_middle[i] + delta_backbone[i] + delta_neck[i])\n",
    "print_frequency_plot(delta_post, \"Postprocessing time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, reset the file with the dictionary\n",
    "handler.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do on MMDeploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_base_ = ['./voxel-detection_dynamic.py', '../../_base_/backends/tensorrt-fp16.py']\n",
    "backend_config = dict(\n",
    "    common_config=dict(max_workspace_size=1 << 30),\n",
    "    model_inputs=[\n",
    "        dict(\n",
    "            input_shapes=dict(\n",
    "                voxels=dict(\n",
    "                    # What goes here and below?\n",
    "                    #   - minimum, optimal, maximum shape of the tensor that is taken as input\n",
    "                    #   - [a, b, c]\n",
    "                    #       - a = number of voxels\n",
    "                    #       - b = max number of points per voxel \n",
    "                    #       - c = dimension of the tensor (4=x,y,z,intensity)\n",
    "                    # Because of the way the voxelization is performed, the same parameters can\n",
    "                    # be used for all of the three cases (voxels, points, coors)\n",
    "                    min_shape=[200, 32, 4],\n",
    "                    opt_shape=[5000, 32, 4],\n",
    "                    max_shape=[40000, 32, 4]),\n",
    "                num_points=dict(\n",
    "                    min_shape=[200],\n",
    "                    opt_shape=[5000],\n",
    "                    max_shape=[40000]),\n",
    "                coors=dict(\n",
    "                    min_shape=[200,  4],\n",
    "                    opt_shape=[5000, 4],\n",
    "                    max_shape=[40000, 4]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notes:\n",
    "#   - tensorrt.py           YES                 1. (strange error)\n",
    "#   - tensorrt-fp16.py      YES                 1. (strange error)\n",
    "#                                                   --> SOLVED by adjusting the \"input_shapes\" dictionary\n",
    "#   - tensorrt-int8.py      does NOT work       2. (cannot find a file)\n",
    "#\n",
    "#   - onnxruntime.py        YES (only .onnx)\n",
    "#   - onnxruntime-fp16.py   YES (only .onnx)    3. (couldnt't find a converter module) \n",
    "#                                                   --> SOLVED by installing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python3 tools/deploy.py configs/mmdet3d/voxel-detection/voxel-detection_tensorrt_dynamic-kitti-32x4.py /home/michele/code/michele_mmdet3d/configs/minerva/CONDENSED_pointpillars_minerva.py /home/michele/code/michele_mmdet3d/work_dirs/pointpillars_minerva/epoch_120.pth /home/michele/code/michele_mmdet3d/data/minerva_polimove/training/velodyne/1723235584511788288.bin --device cuda\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
