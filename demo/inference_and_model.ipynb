{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencer study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various steps\n",
    "1. Import and initialize the inferencer\n",
    "2. Select some images, then **Just inference**\n",
    "3. **Inference + Visualization**\n",
    "4. **Remote** Inference + **Remote** Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# # Print the PYTHONPATH before the modifications\n",
    "# for i, path in enumerate(sys.path):\n",
    "#     print(i, path)\n",
    "# print()\n",
    "\n",
    "# Add the new paths separately\n",
    "# new_path1 = \"/home/michele/.local/lib/python3.10/site-packages/\"\n",
    "# if not new_path1 in sys.path:\n",
    "#     sys.path.insert(1, new_path1)\n",
    "new_path2 = \"/home/michele/code/michele_mmdet3d/\"\n",
    "if not new_path2 in sys.path:\n",
    "    sys.path.insert(1, new_path2)\n",
    "\n",
    "# # Print the PYTHONPATH after the modifications\n",
    "# for i, path in enumerate(sys.path):\n",
    "#     print(i, path)\n",
    "# print()\n",
    "\n",
    "# Print the version of MMCV\n",
    "import mmcv\n",
    "print(mmcv.__version__)\n",
    "print(mmcv.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from mmdet3d.apis import LidarDet3DInferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                           Initialize Inferencer                                               #\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "inferencer = LidarDet3DInferencer(model='/home/michele/code/michele_mmdet3d/configs/minerva/CONDENSED_pointpillars_minerva.py', \n",
    "                                  weights='/home/michele/code/michele_mmdet3d/work_dirs/pointpillars_minerva/epoch_120.pth',\n",
    "                                  show_progress = False)\n",
    "\n",
    "# inferencer = LidarDet3DInferencer(model='/home/michele/code/michele_mmdet3d/configs/minerva/CONDENSED_pointpillars_minerva.py', \n",
    "#                                   weights='/home/michele/code/michele_mmdet3d/work_dirs/pointpillars_minerva/epoch_120.pth',\n",
    "#                                   want_losses=True,\n",
    "#                                   show_progress = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                               Just Inference                                                  #\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Read the files in validation list\n",
    "val_list_txt_file = \"/home/michele/code/michele_mmdet3d/data/minerva_polimove/ImageSets/val.txt\"\n",
    "with open(val_list_txt_file, 'r') as file:\n",
    "    val_file_names = [line.strip() for line in file]\n",
    "\n",
    "# Choose a smaller set of the validation files\n",
    "one_every_n = 1\n",
    "max_number = 1\n",
    "wait_time_default = 10\n",
    "inputs = []\n",
    "for i, name in enumerate(val_file_names):\n",
    "    if i%one_every_n==0 and len(inputs)<max_number:\n",
    "        inputs.append(dict(points=(\"/home/michele/code/michele_mmdet3d/data/minerva_polimove/training/velodyne/\"+name+\".bin\")))\n",
    "\n",
    "print(f\"Total validation point_clouds: {len(val_file_names)}\")\n",
    "print(f\"\\tOne every n: {one_every_n}\")\n",
    "print(f\"\\tMax number: {max_number}\")\n",
    "print(f\"\\tSelected point_clouds: {len(inputs)}\")\n",
    "\n",
    "# Initial format for the inputs was the following:\n",
    "results = []\n",
    "for input in inputs:\n",
    "    results.append(inferencer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                           Inference and Visualize                                             #\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# NOTE by Michele:  The visualization does not work properly if it gets passed a list. So just cycle through the \n",
    "#       \"__call__\" funcion of the inferencer\n",
    "for input in inputs:\n",
    "    inferencer(input, show=True, wait_time = wait_time_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#                                              Remote Inferencer                                                #\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "out_directory = '/home/michele/code/michele_mmdet3d/data/minerva_polimove/remote_inference/'\n",
    "if os.path.exists(out_directory):\n",
    "    shutil.rmtree(out_directory)\n",
    "    print(\"Pre-existing directory removed. Will be created again from scratch\")\n",
    "\n",
    "# NOTE by Michele:  Same as above (need to use the for loop)\n",
    "for input in inputs:\n",
    "    inferencer(input, show=False, out_dir=out_directory)\n",
    "\n",
    "# Visualize the predictions from the saved files\n",
    "local_inferencer = LidarDet3DInferencer(model='/home/michele/code/michele_mmdet3d/configs/minerva/CONDENSED_pointpillars_minerva.py',\n",
    "                                        weights='/home/michele/code/michele_mmdet3d/work_dirs/pointpillars_minerva/epoch_120.pth')\n",
    "\n",
    "saved_predictions = [(out_directory + \"preds/\" + filename) for filename in os.listdir(out_directory + \"preds/\")]\n",
    "local_inferencer.visualize_preds_fromfile(saved_predictions, show=True, wait_time = wait_time_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxelNet(SingleStage3DDetector(Base3DDetector)): Description by ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Base3DDetector` class in MMDetection3D serves as a parent class for 3D detectors, providing a unified interface for 3D object detection models. It inherits from `BaseDetector`, which is part of the 2D detection framework MMDetection, and extends it to handle 3D-specific data and operations. Below, I'll break down its interactions with PyTorch and explain its overall workflow:\n",
    "\n",
    "### 1. **Interaction with PyTorch**\n",
    "The main interactions with PyTorch happen in the `forward` method, which handles different modes of operation: `'tensor'`, `'predict'`, and `'loss'`. These modes control how the model processes inputs and interacts with PyTorch tensors.\n",
    "\n",
    "1. **Forward Pass (`mode='tensor'`)**:  \n",
    "   When `mode='tensor'`, the `forward()` method calls `_forward()`. This is where the network processes the inputs through the various layers defined in the specific detector model (e.g., backbone, neck, head). This is a typical PyTorch `nn.Module` forward pass, which returns raw tensor outputs directly without additional processing.\n",
    "\n",
    "2. **Prediction (`mode='predict'`)**:  \n",
    "   When `mode='predict'`, it processes the inputs and data samples using the `predict()` method. This involves running inference, post-processing, and formatting the results into structured outputs like bounding boxes, scores, and labels. During this stage, PyTorch is used for operations like applying non-maximum suppression (NMS) on 3D bounding boxes and converting model outputs into usable predictions.\n",
    "\n",
    "3. **Loss Calculation (`mode='loss'`)**:  \n",
    "   When `mode='loss'`, it calculates and returns the loss values using the `loss()` method. This mode is used during training. The `inputs` and `data_samples` are passed to the `loss()` function, which computes various losses (e.g., classification, regression) based on ground truth data. The resulting tensors are then used by the optimizer for backpropagation in a typical PyTorch training loop.\n",
    "\n",
    "### 2. **High-Level Workflow of `Base3DDetector`**\n",
    "The overall workflow of the class, when instantiated as part of a detector model, follows these steps:\n",
    "\n",
    "1. **Initialization (`__init__` method)**:\n",
    "   - The class accepts a `data_preprocessor` and an `init_cfg` argument.\n",
    "   - `data_preprocessor` configures preprocessing steps like padding, mean normalization, and standardization for both point cloud and image data.\n",
    "   - `init_cfg` handles initialization settings, such as loading pretrained weights or defining weight initialization schemes.\n",
    "\n",
    "2. **Forward Method (`forward`)**:\n",
    "   The `forward()` method is the unified entry point for different operations. It accepts the following key parameters:\n",
    "   - `inputs`: A dictionary or list of dictionaries containing batch data. It typically includes keys like `points` (for point cloud data) and `imgs` (for image data).\n",
    "   - `data_samples`: Optional annotation data for each sample, such as ground truth 3D bounding boxes.\n",
    "   - `mode`: The mode to run the forward process (`'tensor'`, `'predict'`, or `'loss'`).\n",
    "\n",
    "   Based on the mode, it delegates the call to one of three methods:\n",
    "   - **`loss()`**: Computes the loss for training.\n",
    "   - **`predict()`**: Processes inputs and generates predictions.\n",
    "   - **`_forward()`**: Returns the raw tensor outputs for deeper analysis or debugging.\n",
    "\n",
    "3. **Adding Predictions to Data Samples (`add_pred_to_datasample` method)**:\n",
    "   This utility method formats the model outputs into the structure required for downstream use. It converts raw predictions into the `Det3DDataSample` format, which includes keys like:\n",
    "   - `pred_instances_3d`: Contains 3D bounding boxes (`bboxes_3d`), labels (`labels_3d`), and scores (`scores_3d`).\n",
    "   - `pred_instances`: Similar structure for 2D bounding boxes if applicable.\n",
    "\n",
    "   It creates new `InstanceData` objects for missing 3D or 2D data and adds these predictions back into the `data_samples` list. This structure makes it easier to evaluate and visualize the results later.\n",
    "\n",
    "### 3. **Usage in PyTorch Training and Inference Loops**\n",
    "The `Base3DDetector` class is designed to fit seamlessly into PyTorch's training and inference workflows. During training, it is used as follows:\n",
    "\n",
    "1. **Training**:\n",
    "   - When calling `model(inputs, data_samples, mode='loss')`, the class processes the input data through the network and computes the loss.\n",
    "   - The loss dictionary is used by the optimizer to perform backpropagation and update weights.\n",
    "\n",
    "2. **Inference**:\n",
    "   - When calling `model(inputs, data_samples, mode='predict')`, it produces the post-processed predictions in the format expected by MMDetection3D's evaluation pipeline.\n",
    "   - Predictions are wrapped into `Det3DDataSample` objects, making it easy to evaluate using standard metrics (e.g., mAP, AP40).\n",
    "\n",
    "3. **Debugging and Tensor Operations**:\n",
    "   - When calling `model(inputs, mode='tensor')`, it skips all post-processing and returns the raw tensors directly, allowing for deeper debugging or custom analysis using PyTorch tensor operations.\n",
    "\n",
    "### 4. **Extensibility for New 3D Detection Models**\n",
    "The `Base3DDetector` class serves as a template for implementing new 3D detectors. Derived classes can:\n",
    "- Override `loss()` to implement custom loss functions.\n",
    "- Implement `predict()` for different types of 3D detectors (e.g., voxel-based, point-based).\n",
    "- Customize `_forward()` to define different network architectures.\n",
    "\n",
    "### 5. **Example: How it Fits in an MMDetection3D Pipeline**\n",
    "Consider a typical 3D object detection training pipeline:\n",
    "1. **Data Preprocessing**: The dataset class formats input point clouds and images, passes them through the preprocessor, and provides `inputs` and `data_samples`.\n",
    "2. **Training Loop**: The modelâ€™s `forward()` method is called in `'loss'` mode, computing the losses.\n",
    "3. **Inference Loop**: During testing, the same `forward()` method is called in `'predict'` mode to generate detection results.\n",
    "4. **Result Formatting**: `add_pred_to_datasample()` structures the output for easy evaluation and visualization.\n",
    "\n",
    "In essence, `Base3DDetector` provides the foundation, and specific 3D detectors extend it to implement unique architectures and functionality tailored to their tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Components of the model inside MMDetection3D:** --------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- General type of net is \"VoxelNet\" with the following sub-components:\n",
    "    1. data_preprocessor \n",
    "    2. voxel_encoder\n",
    "    3. middle_encoder\n",
    "    4. backbone\n",
    "    5. neck\n",
    "    6. bbox_head\n",
    "- Better description of the \"3DDetector\" class in the paragraph above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessor (Det3DDataPreprocessor)** ------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- By delving into *VoxelNet --> SingleStage3DDetector --> Base3DDetector --> BaseDetector --> BaseModel*, you can find that the **data_preprocessor** is called every time the *\"run_forward\"* method is called.\n",
    "    - *\"run_forward\"* can be of **mode=predict** in *\"val_step\"* and *\"test_step\"*, or it can be of **mode=loss** in *\"train_step\"*\n",
    "    - The data_preprocessor provides the basic data for the *\"run_forward\"* method to be executed\n",
    "- In our case the data_preprocessor is of class **\"Det3DDataPreprocessor\"**.\n",
    "    - Calls the function *\"voxelize\"* that can be of four types: hard/dynamic/cylindrical/minkunet voxelization. In our case it is **hard** voxelization.\n",
    "    - Performs the voxelization based on the information provided to the dictionary **\"voxel_layer\"** in the configuration file.\n",
    "        - max_voxels\n",
    "        - max_num_points\n",
    "        - point_cloud_range\n",
    "        - voxel_size\n",
    "- The voxelization_layer is of **class VoxelizationByGridShape** which also accepts the flag *\"deterministic = False\"* which may speed up the process. <mark>It is worth a try</mark>\n",
    "- There is a way to perform **dynamic voxelization**, and also a VoxelEncoder that supports dynamic voxelization (same file as the other). <mark>It is worth a try</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Voxel Encoder (PillarFeatureNet)** ---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Performs the first processing of the voxels.\n",
    "    - Takes as input a tensor of shape (NxMxC) with N=n. of pillars, M=n. of max points per pillar, C=n. of features per point (in our case 4).\n",
    "    - To the input some parameters can be added, such as *cluster center distance* (from centroid), *voxel center distance* (from voxel centre), and *distance from the origin* (from voxel origin). So the new input becomes (NxMxC_augmented). This method is called **feature augmentation**.\n",
    "    - For each pillar, **pooling** is performed (either max or average) to extract the main features of the pillar. The number of features is defined in the parameter **\"feat_channels\"**. <mark>May try to modify **number of features** and also **type of pooling**</mark>\n",
    "    - The output is (NxC_features) where C_features is exactly the parameter *\"feat_channels\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Middle Encoder (PointPillarsScatter)** -----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Simply takes the voxels as listed in a **(NxC_features)** tensor, and turns them into a **(1xC_featuresxN_yxN_x)** tensor.\n",
    "- Essentially, takes the pillars **from a 1-D representation** and orders them **to a grid-like structure** which can be used **for CNN-like computations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Backbone (SECOND)** ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Convolution in 3 steps as explained in the following.\n",
    "    - From (C, X, Y)        to      (C, X/2, Y/2)\n",
    "    - From (C, X/2, Y/2)    to      (2C, X/4, Y/4)\n",
    "    - From (2C, X/4, Y/4)   to      (4C, X/8, Y/8)\n",
    "- All of these layers proceed in **PARALLEL**, and they will be decovoluted and then concatenated in the neck (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neck (SECOND FPN)** ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Another standard FPN: Feature Pyramid Network. \n",
    "    - There are a down-sampling and then up-sampling that proceed in parallel with **lateral connections** that are then merged together.\n",
    "    - Multi-scale features allow to get spacial resolution from non-scaled images, and feature richness by scaled images.\n",
    "- The up-sampling works like the following.\n",
    "    - From (C, X/2, Y/2)    to      (2C, X/2, Y/2)\n",
    "    - From (2C, X/4, Y/4)   to      (2C, X/2, Y/2)\n",
    "    - From (4C, X/8, Y/8)   to      (2C, X/2, Y/2)\n",
    "- So at the end there are 2C-thick layers for three times, and they are sticked together to get (6C, X/2, Y/2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BBox Head (Anchor3DHead)** -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Takes as input the feature map, and outputs the final results for prediction and training.\n",
    "    - For just **INFERENCE** the outputs are\n",
    "        - Predicted BBox parameters (7 values).\n",
    "        - Classification scores for each BBox, a value for each class.\n",
    "        - Direction classification.\n",
    "    - For **TRAINING** the outputs are instead\n",
    "        - Classification scores, BBox offset from center, classification predictions.\n",
    "        - Loss values for the back-propagation, so *loss_cls/loss_bbox/loss_dir*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
